{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1dsidLuUQ-l"
      },
      "source": [
        "# Multi Digit Number Recognition with SVHN\n",
        "\n",
        "This notebook implements multi digit number recognition using SVHN dataset that will be used to recognize house numbers at the streets. It can be considered as second version of the previous multi digit recognition which uses MNIST database. Keras and Tensorflow libraries are used to build the recognizer.\n",
        "This recognizer extracts digit from the image using Convolutional Neural Network Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QUuHcyyF5iq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "from six.moves import range\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4a-ERiYqFl8",
        "outputId": "77766048-3a34-4259-dcb5-7c08114ee660"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
            "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
            "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
            "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
            "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
            "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
            "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from skimage.color import rgb2gray\n",
        "from keras.optimizers import SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaQvdGcyqFl-",
        "outputId": "88ab0c9a-146c-49ed-f205-037e900dd74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.12.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
            "Collecting tensorflow-intel==2.12.0 (from tensorflow)\n",
            "  Using cached tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl (272.9 MB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.3.3)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
            "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached jax-0.4.8-py3-none-any.whl\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.22.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached grpcio-1.54.0-cp311-cp311-win_amd64.whl (4.1 MB)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached tensorboard-2.12.2-py3-none-any.whl (5.6 MB)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.40.0)\n",
            "Collecting ml-dtypes>=0.0.3 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached ml_dtypes-0.1.0-cp311-cp311-win_amd64.whl (120 kB)\n",
            "Requirement already satisfied: scipy>=1.7 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.0)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "Collecting requests<3,>=2.21.0 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached requests-2.29.0-py3-none-any.whl (62 kB)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.3.2)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached charset_normalizer-3.1.0-cp311-cp311-win_amd64.whl (96 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.15)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
            "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\abhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
            "Installing collected packages: rsa, pyasn1-modules, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, charset-normalizer, certifi, cachetools, astunparse, absl-py, requests, jax, google-auth, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
            "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 certifi-2022.12.7 charset-normalizer-3.1.0 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.0 jax-0.4.8 keras-2.12.0 markdown-3.4.3 ml-dtypes-0.1.0 opt-einsum-3.3.0 pyasn1-modules-0.3.0 requests-2.29.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.2 tensorflow-2.12.0 tensorflow-intel-2.12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\abhi\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\abhi\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script normalizer.exe is installed in 'C:\\Users\\abhi\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\abhi\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\abhi\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\abhi\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --upgrade tensorflow --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_giURzYGjWO"
      },
      "outputs": [],
      "source": [
        "# Setting the random seed so that the results are reproducible. \n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifZinCXN3yk1"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4mpLO-oqFmA",
        "outputId": "11b31efb-e369-4b4d-b807-6537c57aae0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set (27401, 64, 64, 1) (27401, 5)\n",
            "Validation set (3000, 64, 64, 1) (3000, 5)\n",
            "Test set (13068, 64, 64, 1) (13068, 5)\n"
          ]
        }
      ],
      "source": [
        "filename = 'data/SVHN_multi_grey.h5'\n",
        "\n",
        "data = h5py.File(filename, 'r')\n",
        "train_dataset = np.array(data[\"train_dataset\"])\n",
        "train_labels = np.array(data[\"train_labels\"])\n",
        "test_dataset = np.array(data[\"test_dataset\"])\n",
        "test_labels = np.array(data[\"test_labels\"])\n",
        "valid_dataset = np.array(data[\"valid_dataset\"])\n",
        "valid_labels = np.array(data[\"valid_labels\"])\n",
        "\n",
        "print('Training set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "UK68ofOrHnI1",
        "outputId": "266b75bd-8a8b-402c-dac9-e2de6cc5a6f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label for image: [ 2  6 10 10 10]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_dataset[70])\n",
        "\n",
        "print(f\"Label for image: {train_labels[70]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOQpSYQNHKIu"
      },
      "source": [
        "## Convert Labels\n",
        "This function converts each digit label to one-hot array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slWvJtn2Kts1"
      },
      "outputs": [],
      "source": [
        "# Converting labels to One-hot representations of shape (set_size, digits, classes)\n",
        "possible_classes = 11\n",
        "\n",
        "def convert_labels(labels):\n",
        "    \n",
        "    # As per Keras conventions, the multiple labels need to be of the form [array_digit1,...5]\n",
        "    # Each digit array will be of shape (60000,11)\n",
        "        \n",
        "    # Declare output ndarrays\n",
        "    # 5 for digits, 11 for possible classes  \n",
        "    dig0_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    dig1_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    dig2_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    dig3_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    dig4_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    \n",
        "    for index,label in enumerate(labels):\n",
        "        \n",
        "        # Using utils.np_utils from keras to OHE the labels in the image\n",
        "        dig0_arr[index,:] = utils.to_categorical(label[0],possible_classes)\n",
        "        dig1_arr[index,:] = utils.to_categorical(label[1],possible_classes)\n",
        "        dig2_arr[index,:] = utils.to_categorical(label[2],possible_classes)\n",
        "        dig3_arr[index,:] = utils.to_categorical(label[3],possible_classes)\n",
        "        dig4_arr[index,:] = utils.to_categorical(label[4],possible_classes)\n",
        "        \n",
        "    return [dig0_arr,dig1_arr,dig2_arr,dig3_arr,dig4_arr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17m-itv9Lizo"
      },
      "outputs": [],
      "source": [
        "train_labels = convert_labels(train_labels)\n",
        "test_labels = convert_labels(test_labels)\n",
        "valid_labels = convert_labels(valid_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NOjPwpQTLmAn",
        "outputId": "20a54080-4dc1-4735-a943-9925a65a5777"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(27401, 11)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Checking the shape of the OHE array for the first digit position\n",
        "np.shape(train_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe8PQKllIT2N"
      },
      "source": [
        "## Prepare Data for Keras\n",
        "Reshape image data to be processed by Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTQg2PaWLzc0"
      },
      "outputs": [],
      "source": [
        "def prep_data_keras(img_data):\n",
        "    \n",
        "    # Reshaping data for keras, with tensorflow as backend\n",
        "    img_data = img_data.reshape(len(img_data), 64, 64, 1)\n",
        "    \n",
        "    # Converting everything to floats\n",
        "    img_data = img_data.astype('float32')\n",
        "    \n",
        "    # Normalizing values between 0 and 1\n",
        "    #img_data /= 255\n",
        "    \n",
        "    return img_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHUhGAErL--5"
      },
      "outputs": [],
      "source": [
        "train_images = prep_data_keras(train_dataset)\n",
        "test_images = prep_data_keras(test_dataset)\n",
        "valid_images = prep_data_keras(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "D2p7Py9wMBGw",
        "outputId": "f62d3c15-cccd-4a20-bf10-f257141fbfad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(27401, 64, 64, 1)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "tyXraB2MMQvR",
        "outputId": "881da3a9-e576-41d2-ee13-79fc74012129"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13068, 64, 64, 1)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INF-j_J_MTi_"
      },
      "outputs": [],
      "source": [
        "# Importing relevant keras modules\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adam, Adagrad\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj66iuNtLfJu"
      },
      "source": [
        "## Build Model\n",
        "Build Deep Learning model to process data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13y5-nN1qFmJ"
      },
      "source": [
        "# Performance of model on different layers of CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjEpaTcFqFmJ"
      },
      "source": [
        "## (1) Conv1, Conv2, pooling1, Dropout, Flatten, FC,Dropout, Prediction(8 layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEe8H3_EqFmK"
      },
      "outputs": [],
      "source": [
        "# Building the model\n",
        "\n",
        "batch_size = 128\n",
        "nb_classes = 11\n",
        "# nb_epoch = 24\n",
        "nb_epoch = 10\n",
        "\n",
        "# image input dimensions\n",
        "img_rows = 64\n",
        "img_cols = 64\n",
        "img_channels = 1\n",
        "\n",
        "# number of convulation filters to use\n",
        "nb_filters = 32\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "# defining the input\n",
        "inputs = Input(shape=(img_rows, img_cols, img_channels))\n",
        "\n",
        "# Model taken from keras example.\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(inputs)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "\n",
        "cov_out = Flatten()(cov)\n",
        "\n",
        "\n",
        "# Dense Layers\n",
        "cov2 = Dense(2056, activation='relu')(cov_out)\n",
        "cov2 = Dropout(0.3)(cov2)\n",
        "\n",
        "\n",
        "# Prediction layers\n",
        "c0 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c1 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c2 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c3 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c4 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "\n",
        "# Defining the model\n",
        "model = Model(inputs=inputs,outputs=[c0,c1,c2,c3,c4])\n",
        "# print (model.summary())\n",
        "\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "    \n",
        "# Fitting the model\n",
        "model.fit(train_images,train_labels,batch_size=batch_size,epochs=nb_epoch,verbose=1,\n",
        "          validation_data=(valid_images, valid_labels), callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U0OqevtqFmK"
      },
      "source": [
        "## (2) Conv1, Conv2, pooling1,Dropout, Conv3,Conv4, Pooling2, Dropout, Flatten, FC,Dropout, Prediction(12 layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfgvBasAqFmL"
      },
      "outputs": [],
      "source": [
        "# Building the model\n",
        "\n",
        "# below variables are already defined in starting of model building code, so it is not required to define here\n",
        "# batch_size = 128\n",
        "# nb_classes = 11\n",
        "# # nb_epoch = 24\n",
        "# nb_epoch = 10\n",
        "\n",
        "# # image input dimensions\n",
        "# img_rows = 64\n",
        "# img_cols = 64\n",
        "# img_channels = 1\n",
        "\n",
        "# # number of convulation filters to use\n",
        "# nb_filters = 32\n",
        "# # size of pooling area for max pooling\n",
        "# pool_size = (2, 2)\n",
        "# # convolution kernel size\n",
        "# kernel_size = (3, 3)\n",
        "\n",
        "# # defining the input\n",
        "# inputs = Input(shape=(img_rows, img_cols, img_channels))\n",
        "\n",
        "# Model taken from keras example.\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(inputs)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov = Conv2D((nb_filters * 2),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 2),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cov_out = Flatten()(cov)\n",
        "\n",
        "\n",
        "# Dense Layers\n",
        "cov2 = Dense(2056, activation='relu')(cov_out)\n",
        "cov2 = Dropout(0.3)(cov2)\n",
        "\n",
        "\n",
        "\n",
        "# Prediction layers\n",
        "c0 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c1 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c2 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c3 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c4 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "\n",
        "# Defining the model\n",
        "model2 = Model(inputs=inputs,outputs=[c0,c1,c2,c3,c4])\n",
        "# print (model.summary())\n",
        "\n",
        "\n",
        "# Compiling the model\n",
        "model2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "    \n",
        "# Fitting the model\n",
        "model2.fit(train_images,train_labels,batch_size=batch_size,epochs=nb_epoch,verbose=1,\n",
        "          validation_data=(valid_images, valid_labels), callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG1B2GKMqFmL"
      },
      "source": [
        "## (3) Conv1, Conv2, pooling1, Dropout, Conv3, Conv4, Pooling2, Dropout, Conv5, Conv6, Pooling3, Dropout, Conv7, Conv8, Pooling4, Dropout, Flatten, FC, Dropout, Prediction (20 layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2281
        },
        "id": "ixTneFcAM_BN",
        "outputId": "04c3019d-ac52-4b8a-9f8b-579e78b34690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "215/215 [==============================] - 678s 3s/step - loss: 6.1771 - dense_7_loss: 2.1182 - dense_8_loss: 2.4377 - dense_9_loss: 1.3081 - dense_10_loss: 0.2971 - dense_11_loss: 0.0161 - dense_7_accuracy: 0.2613 - dense_8_accuracy: 0.1720 - dense_9_accuracy: 0.6936 - dense_10_accuracy: 0.9534 - dense_11_accuracy: 0.9962 - val_loss: 5.8441 - val_dense_7_loss: 2.0100 - val_dense_8_loss: 2.3141 - val_dense_9_loss: 1.2307 - val_dense_10_loss: 0.2868 - val_dense_11_loss: 0.0027 - val_dense_7_accuracy: 0.3310 - val_dense_8_accuracy: 0.2063 - val_dense_9_accuracy: 0.6947 - val_dense_10_accuracy: 0.9540 - val_dense_11_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 677s 3s/step - loss: 4.0664 - dense_7_loss: 1.3280 - dense_8_loss: 1.6233 - dense_9_loss: 0.9109 - dense_10_loss: 0.2004 - dense_11_loss: 0.0038 - dense_7_accuracy: 0.5500 - dense_8_accuracy: 0.4494 - dense_9_accuracy: 0.7291 - dense_10_accuracy: 0.9578 - dense_11_accuracy: 0.9997 - val_loss: 4.7376 - val_dense_7_loss: 1.5584 - val_dense_8_loss: 2.0125 - val_dense_9_loss: 0.9395 - val_dense_10_loss: 0.2269 - val_dense_11_loss: 4.1462e-04 - val_dense_7_accuracy: 0.4893 - val_dense_8_accuracy: 0.2700 - val_dense_9_accuracy: 0.7187 - val_dense_10_accuracy: 0.9540 - val_dense_11_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 635s 3s/step - loss: 2.1270 - dense_7_loss: 0.6339 - dense_8_loss: 0.7892 - dense_9_loss: 0.5535 - dense_10_loss: 0.1472 - dense_11_loss: 0.0032 - dense_7_accuracy: 0.7965 - dense_8_accuracy: 0.7476 - dense_9_accuracy: 0.8255 - dense_10_accuracy: 0.9607 - dense_11_accuracy: 0.9997 - val_loss: 1.4897 - val_dense_7_loss: 0.4162 - val_dense_8_loss: 0.5213 - val_dense_9_loss: 0.4162 - val_dense_10_loss: 0.1352 - val_dense_11_loss: 7.7720e-04 - val_dense_7_accuracy: 0.8670 - val_dense_8_accuracy: 0.8420 - val_dense_9_accuracy: 0.8727 - val_dense_10_accuracy: 0.9607 - val_dense_11_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "215/215 [==============================] - 631s 3s/step - loss: 1.5384 - dense_7_loss: 0.4604 - dense_8_loss: 0.5546 - dense_9_loss: 0.3982 - dense_10_loss: 0.1229 - dense_11_loss: 0.0023 - dense_7_accuracy: 0.8579 - dense_8_accuracy: 0.8273 - dense_9_accuracy: 0.8771 - dense_10_accuracy: 0.9642 - dense_11_accuracy: 0.9997 - val_loss: 1.1561 - val_dense_7_loss: 0.3615 - val_dense_8_loss: 0.3970 - val_dense_9_loss: 0.2845 - val_dense_10_loss: 0.1130 - val_dense_11_loss: 8.4247e-05 - val_dense_7_accuracy: 0.8837 - val_dense_8_accuracy: 0.8750 - val_dense_9_accuracy: 0.9143 - val_dense_10_accuracy: 0.9673 - val_dense_11_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "215/215 [==============================] - 637s 3s/step - loss: 1.2755 - dense_7_loss: 0.3836 - dense_8_loss: 0.4555 - dense_9_loss: 0.3270 - dense_10_loss: 0.1068 - dense_11_loss: 0.0025 - dense_7_accuracy: 0.8822 - dense_8_accuracy: 0.8591 - dense_9_accuracy: 0.8996 - dense_10_accuracy: 0.9680 - dense_11_accuracy: 0.9997 - val_loss: 1.0925 - val_dense_7_loss: 0.3270 - val_dense_8_loss: 0.3833 - val_dense_9_loss: 0.2536 - val_dense_10_loss: 0.1279 - val_dense_11_loss: 7.9152e-04 - val_dense_7_accuracy: 0.8993 - val_dense_8_accuracy: 0.8893 - val_dense_9_accuracy: 0.9227 - val_dense_10_accuracy: 0.9640 - val_dense_11_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "215/215 [==============================] - 630s 3s/step - loss: 1.0955 - dense_7_loss: 0.3337 - dense_8_loss: 0.3899 - dense_9_loss: 0.2764 - dense_10_loss: 0.0934 - dense_11_loss: 0.0020 - dense_7_accuracy: 0.8976 - dense_8_accuracy: 0.8809 - dense_9_accuracy: 0.9147 - dense_10_accuracy: 0.9718 - dense_11_accuracy: 0.9997 - val_loss: 0.9085 - val_dense_7_loss: 0.2586 - val_dense_8_loss: 0.3428 - val_dense_9_loss: 0.2206 - val_dense_10_loss: 0.0864 - val_dense_11_loss: 1.7486e-04 - val_dense_7_accuracy: 0.9167 - val_dense_8_accuracy: 0.9040 - val_dense_9_accuracy: 0.9367 - val_dense_10_accuracy: 0.9733 - val_dense_11_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "215/215 [==============================] - 636s 3s/step - loss: 1.0014 - dense_7_loss: 0.3060 - dense_8_loss: 0.3594 - dense_9_loss: 0.2509 - dense_10_loss: 0.0831 - dense_11_loss: 0.0020 - dense_7_accuracy: 0.9030 - dense_8_accuracy: 0.8909 - dense_9_accuracy: 0.9231 - dense_10_accuracy: 0.9751 - dense_11_accuracy: 0.9997 - val_loss: 0.8326 - val_dense_7_loss: 0.2369 - val_dense_8_loss: 0.2838 - val_dense_9_loss: 0.2151 - val_dense_10_loss: 0.0962 - val_dense_11_loss: 5.7366e-04 - val_dense_7_accuracy: 0.9210 - val_dense_8_accuracy: 0.9253 - val_dense_9_accuracy: 0.9377 - val_dense_10_accuracy: 0.9700 - val_dense_11_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "215/215 [==============================] - 628s 3s/step - loss: 0.8950 - dense_7_loss: 0.2854 - dense_8_loss: 0.3153 - dense_9_loss: 0.2206 - dense_10_loss: 0.0720 - dense_11_loss: 0.0018 - dense_7_accuracy: 0.9122 - dense_8_accuracy: 0.9038 - dense_9_accuracy: 0.9319 - dense_10_accuracy: 0.9783 - dense_11_accuracy: 0.9997 - val_loss: 0.7939 - val_dense_7_loss: 0.2243 - val_dense_8_loss: 0.2995 - val_dense_9_loss: 0.1962 - val_dense_10_loss: 0.0735 - val_dense_11_loss: 3.8608e-04 - val_dense_7_accuracy: 0.9290 - val_dense_8_accuracy: 0.9170 - val_dense_9_accuracy: 0.9417 - val_dense_10_accuracy: 0.9807 - val_dense_11_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "215/215 [==============================] - 629s 3s/step - loss: 0.8164 - dense_7_loss: 0.2546 - dense_8_loss: 0.2949 - dense_9_loss: 0.2011 - dense_10_loss: 0.0641 - dense_11_loss: 0.0017 - dense_7_accuracy: 0.9207 - dense_8_accuracy: 0.9091 - dense_9_accuracy: 0.9371 - dense_10_accuracy: 0.9800 - dense_11_accuracy: 0.9997 - val_loss: 0.7681 - val_dense_7_loss: 0.2237 - val_dense_8_loss: 0.2662 - val_dense_9_loss: 0.2074 - val_dense_10_loss: 0.0706 - val_dense_11_loss: 1.8468e-04 - val_dense_7_accuracy: 0.9313 - val_dense_8_accuracy: 0.9247 - val_dense_9_accuracy: 0.9407 - val_dense_10_accuracy: 0.9813 - val_dense_11_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "215/215 [==============================] - 631s 3s/step - loss: 0.7723 - dense_7_loss: 0.2444 - dense_8_loss: 0.2786 - dense_9_loss: 0.1880 - dense_10_loss: 0.0602 - dense_11_loss: 0.0011 - dense_7_accuracy: 0.9243 - dense_8_accuracy: 0.9152 - dense_9_accuracy: 0.9430 - dense_10_accuracy: 0.9807 - dense_11_accuracy: 0.9998 - val_loss: 0.8106 - val_dense_7_loss: 0.2471 - val_dense_8_loss: 0.2952 - val_dense_9_loss: 0.2016 - val_dense_10_loss: 0.0662 - val_dense_11_loss: 4.1008e-04 - val_dense_7_accuracy: 0.9227 - val_dense_8_accuracy: 0.9197 - val_dense_9_accuracy: 0.9383 - val_dense_10_accuracy: 0.9823 - val_dense_11_accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x25d0689cd50>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Building the model\n",
        "\n",
        "# below variables are already defined in starting of model building code, so it is not required to define here\n",
        "batch_size = 128\n",
        "nb_classes = 11\n",
        "# nb_epoch = 24\n",
        "nb_epoch = 10\n",
        "\n",
        "# image input dimensions\n",
        "img_rows = 64\n",
        "img_cols = 64\n",
        "img_channels = 1\n",
        "\n",
        "# number of convulation filters to use\n",
        "nb_filters = 32\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "# defining the input\n",
        "inputs = Input(shape=(img_rows, img_cols, img_channels))\n",
        "\n",
        "# Model taken from keras example.\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(inputs)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov = Conv2D((nb_filters * 2),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 2),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "\n",
        "cov = Conv2D((nb_filters * 4),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 4),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov = Conv2D((nb_filters * 8),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 8),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov_out = Flatten()(cov)\n",
        "\n",
        "\n",
        "# Dense Layers\n",
        "cov2 = Dense(2056, activation='relu')(cov_out)\n",
        "cov2 = Dropout(0.3)(cov2)\n",
        "\n",
        "\n",
        "\n",
        "# Prediction layers\n",
        "c0 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c1 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c2 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c3 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c4 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "\n",
        "# Defining the model\n",
        "model3 = Model(inputs=inputs,outputs=[c0,c1,c2,c3,c4])\n",
        "# print (model.summary())\n",
        "\n",
        "\n",
        "# Compiling the model\n",
        "model3.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "    \n",
        "# Fitting the model\n",
        "model3.fit(train_images,train_labels,batch_size=batch_size,epochs=nb_epoch,verbose=1,\n",
        "          validation_data=(valid_images, valid_labels), callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJAi62f2qFmN"
      },
      "source": [
        "# Performance of model for different optimizers in 20 layers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeMTUOrrqFmN"
      },
      "source": [
        "## (1) Stochastic Gradiant Descent with momentum "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vIv6xfWqFmN"
      },
      "outputs": [],
      "source": [
        "# Building the model\n",
        "\n",
        "# below variables are already defined in starting of model building code, so it is not required to define here\n",
        "\n",
        "# batch_size = 128\n",
        "# nb_classes = 11\n",
        "# # nb_epoch = 24\n",
        "# nb_epoch = 10\n",
        "\n",
        "# # image input dimensions\n",
        "# img_rows = 64\n",
        "# img_cols = 64\n",
        "# img_channels = 1\n",
        "\n",
        "# # number of convulation filters to use\n",
        "# nb_filters = 32\n",
        "# # size of pooling area for max pooling\n",
        "# pool_size = (2, 2)\n",
        "# # convolution kernel size\n",
        "# kernel_size = (3, 3)\n",
        "\n",
        "# # defining the input\n",
        "# inputs = Input(shape=(img_rows, img_cols, img_channels))\n",
        "\n",
        "# Model taken from keras example.\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(inputs)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov = Conv2D((nb_filters * 2),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 2),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "\n",
        "cov = Conv2D((nb_filters * 4),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 4),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov = Conv2D((nb_filters * 8),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 8),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov_out = Flatten()(cov)\n",
        "\n",
        "\n",
        "# Dense Layers\n",
        "cov2 = Dense(2056, activation='relu')(cov_out)\n",
        "cov2 = Dropout(0.3)(cov2)\n",
        "\n",
        "\n",
        "\n",
        "# Prediction layers\n",
        "c0 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c1 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c2 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c3 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c4 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "\n",
        "# Defining the model\n",
        "model4 = Model(inputs=inputs,outputs=[c0,c1,c2,c3,c4])\n",
        "# print (model.summary())\n",
        "\n",
        "\n",
        "model4.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "    \n",
        "# Fitting the model\n",
        "model4.fit(train_images,train_labels,batch_size=batch_size,epochs=nb_epoch,verbose=1,\n",
        "          validation_data=(valid_images, valid_labels), callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4s_7g5RqFmO"
      },
      "source": [
        "## (2) Adagrad "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuX9vIDSqFmO"
      },
      "outputs": [],
      "source": [
        "# Building the model\n",
        "\n",
        "# below variables are already defined in starting of model building code, so it is not required to define here\n",
        "# batch_size = 128\n",
        "# nb_classes = 11\n",
        "# # nb_epoch = 24\n",
        "# nb_epoch = 10\n",
        "\n",
        "# # image input dimensions\n",
        "# img_rows = 64\n",
        "# img_cols = 64\n",
        "# img_channels = 1\n",
        "\n",
        "# # number of convulation filters to use\n",
        "# nb_filters = 32\n",
        "# # size of pooling area for max pooling\n",
        "# pool_size = (2, 2)\n",
        "# # convolution kernel size\n",
        "# kernel_size = (3, 3)\n",
        "\n",
        "# # defining the input\n",
        "# inputs = Input(shape=(img_rows, img_cols, img_channels))\n",
        "\n",
        "# Model taken from keras example.\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(inputs)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov = Conv2D((nb_filters * 2),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 2),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "\n",
        "cov = Conv2D((nb_filters * 4),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 4),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov = Conv2D((nb_filters * 8),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 8),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov_out = Flatten()(cov)\n",
        "\n",
        "\n",
        "# Dense Layers\n",
        "cov2 = Dense(2056, activation='relu')(cov_out)\n",
        "cov2 = Dropout(0.3)(cov2)\n",
        "\n",
        "\n",
        "\n",
        "# Prediction layers\n",
        "c0 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c1 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c2 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c3 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c4 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "\n",
        "# Defining the model\n",
        "model5 = Model(inputs=inputs,outputs=[c0,c1,c2,c3,c4])\n",
        "# print (model.summary())\n",
        "\n",
        "\n",
        "model5.compile(loss='categorical_crossentropy', optimizer=Adagrad(lr=0.01), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "    \n",
        "# Fitting the model\n",
        "model5.fit(train_images,train_labels,batch_size=batch_size,epochs=nb_epoch,verbose=1,\n",
        "          validation_data=(valid_images, valid_labels), callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJy1wj2wqFmP"
      },
      "source": [
        "# (3) RMSprop "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFZpe5BYqFmP"
      },
      "outputs": [],
      "source": [
        "# Building the model\n",
        "\n",
        "# below variables are already defined in starting of model building code, so it is not required to define here\n",
        "# batch_size = 128\n",
        "# nb_classes = 11\n",
        "# # nb_epoch = 24\n",
        "# nb_epoch = 10\n",
        "\n",
        "# # image input dimensions\n",
        "# img_rows = 64\n",
        "# img_cols = 64\n",
        "# img_channels = 1\n",
        "\n",
        "# # number of convulation filters to use\n",
        "# nb_filters = 32\n",
        "# # size of pooling area for max pooling\n",
        "# pool_size = (2, 2)\n",
        "# # convolution kernel size\n",
        "# kernel_size = (3, 3)\n",
        "\n",
        "# defining the input\n",
        "# inputs = Input(shape=(img_rows, img_cols, img_channels))\n",
        "\n",
        "# Model taken from keras example.\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(inputs)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D(nb_filters,kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov = Conv2D((nb_filters * 2),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 2),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "\n",
        "cov = Conv2D((nb_filters * 4),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 4),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov = Conv2D((nb_filters * 8),kernel_size=(kernel_size[0],kernel_size[1]), use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = Conv2D((nb_filters * 8),kernel_size=(kernel_size[0],kernel_size[1]),padding='same', use_bias=False)(cov)\n",
        "cov = BatchNormalization()(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size)(cov)\n",
        "cov = Dropout(0.3)(cov)\n",
        "\n",
        "cov_out = Flatten()(cov)\n",
        "\n",
        "\n",
        "# Dense Layers\n",
        "cov2 = Dense(2056, activation='relu')(cov_out)\n",
        "cov2 = Dropout(0.3)(cov2)\n",
        "\n",
        "\n",
        "\n",
        "# Prediction layers\n",
        "c0 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c1 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c2 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c3 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c4 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "\n",
        "# Defining the model\n",
        "model6 = Model(inputs=inputs,outputs=[c0,c1,c2,c3,c4])\n",
        "# print (model.summary())\n",
        "\n",
        "\n",
        "model6.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "    \n",
        "# Fitting the model\n",
        "model6.fit(train_images,train_labels,batch_size=batch_size,epochs=nb_epoch,verbose=1,\n",
        "          validation_data=(valid_images, valid_labels), callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1gl0EeoPo6k",
        "outputId": "28ebc314-b822-4693-e21c-666fe26e03e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "409/409 [==============================] - 61s 136ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions =  model.predict(test_images)\n",
        "predictions2 = model2.predict(test_images)\n",
        "predictions3 = model3.predict(test_images)\n",
        "predictions4 = model4.predict(test_images)\n",
        "predictions5 = model5.predict(test_images)\n",
        "predictions6 = model6.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ijyU9tJ3Psbr",
        "outputId": "e1f619a4-4aa2-4e8c-dbb4-e869dd5da284"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 13068, 11)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(np.shape(predictions))\n",
        "print(np.shape(predictions2))\n",
        "print(np.shape(predictions3))\n",
        "print(np.shape(predictions4))\n",
        "print(np.shape(predictions5))\n",
        "print(np.shape(predictions6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIPcpJ9qqFmR",
        "outputId": "ccc1416c-94ca-4499-f7d8-71f6c809a108"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/cnn_model\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/cnn_model\\assets\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Directory path\n",
        "directory = 'saved_model'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "    \n",
        "model.save('saved_model/cnn_model')\n",
        "model2.save('saved_model/cnn_model2')\n",
        "model3.save('saved_model/cnn_model3')\n",
        "model4.save('saved_model/cnn_model4')\n",
        "model5.save('saved_model/cnn_model5')\n",
        "model6.save('saved_model/cnn_model6')\n",
        "\n",
        "# new_model = tf.keras.models.load_model('saved_model/cnn_model')\n",
        "# new_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8gkvmbWLoY1"
      },
      "source": [
        "## Calculate Accuracy\n",
        "Custom accuracy calculation for individual digits and the whole sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHb7NM2sQZ3U"
      },
      "outputs": [],
      "source": [
        "def calculate_acc(predictions,real_labels):\n",
        "    \n",
        "    individual_counter = 0\n",
        "    global_sequence_counter = 0\n",
        "    coverage_counter = 0\n",
        "    confidence = 0.7\n",
        "    for i in range(0,len(predictions[0])):\n",
        "        # Reset sequence counter at the start of each image\n",
        "        sequence_counter = 0 \n",
        "        \n",
        "        for j in range(0,5):\n",
        "            \n",
        "            if np.argmax(predictions[j][i]) == np.argmax(real_labels[j][i]):\n",
        "                individual_counter += 1\n",
        "                sequence_counter += 1\n",
        "            if predictions[j][i][np.argmax(predictions[j][i])] >= confidence:\n",
        "                coverage_counter += 1\n",
        "        \n",
        "        if sequence_counter == 5:\n",
        "            global_sequence_counter += 1\n",
        "         \n",
        "    ind_accuracy = individual_counter / float(len(predictions[0]) * 5)\n",
        "    global_accuracy = global_sequence_counter / float(len(predictions[0]))\n",
        "    coverage = coverage_counter / float(len(predictions[0]) * 5)\n",
        "    \n",
        "    return ind_accuracy,global_accuracy, coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZSUiAerQ7tq"
      },
      "outputs": [],
      "source": [
        "ind_acc, glob_acc, coverage = calculate_acc(predictions, test_labels)\n",
        "ind_acc2, glob_acc2, coverage2 = calculate_acc(predictions2, test_labels)\n",
        "ind_acc3, glob_acc3, coverage3 = calculate_acc(predictions3, test_labels)\n",
        "ind_acc4, glob_acc4, coverage4 = calculate_acc(predictions4, test_labels)\n",
        "ind_acc5, glob_acc5, coverage5 = calculate_acc(predictions5, test_labels)\n",
        "ind_acc6, glob_acc6, coverage6 = calculate_acc(predictions6, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "_I57NhTmRu2P",
        "outputId": "a2e68d92-a730-4ff4-a00e-0b75ea1a1a3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The individual accuracy is 95.78206305479033 %\n",
            "The sequence prediction accuracy is 83.70829507193143 %\n",
            "The coverage is 94.47352310988674 %\n"
          ]
        }
      ],
      "source": [
        "# these results are for cnn with 20 layers and Adam optimizer, which is better model out of all models\n",
        "print(\"The individual accuracy is {} %\".format(ind_acc3 * 100))\n",
        "print(\"The sequence prediction accuracy is {} %\".format(glob_acc3 * 100))\n",
        "print(\"The coverage is {} %\".format(coverage3 * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking performance over different layers architecture of CNN\n",
        "print(\"The individual accuracy for 8 layers is {} %\".format(ind_acc * 100))\n",
        "print(\"The sequence prediction accuracy for 8 layers is {} %\".format(glob_acc * 100))\n",
        "print(\"The coverage for 8 layers is {} %\".format(coverage * 100))\n",
        "print(\"The individual accuracy for 12 layers is {} %\".format(ind_acc2 * 100))\n",
        "print(\"The sequence prediction accuracy for 12 layers is {} %\".format(glob_acc2 * 100))\n",
        "print(\"The coverage for 12 layers is {} %\".format(coverage2 * 100))\n",
        "print(\"The individual accuracy for 20 layers is {} %\".format(ind_acc3 * 100))\n",
        "print(\"The sequence prediction accuracy for 20 layers is {} %\".format(glob_acc3 * 100))\n",
        "print(\"The coverage for 20 layers is {} %\".format(coverage3 * 100))\n",
        "\n",
        "# checking performance over different optimizers with 20 layers architechture\n",
        "print(\"The individual accuracy for SGD with momentum  is {} %\".format(ind_acc4 * 100))\n",
        "print(\"The sequence prediction accuracy for SGD with momentum is {} %\".format(glob_acc4 * 100))\n",
        "print(\"The coverage for SGD with momentum is {} %\".format(coverage4 * 100))\n",
        "print(\"The individual accuracy for Adagrad is {} %\".format(ind_acc5 * 100))\n",
        "print(\"The sequence prediction accuracy for 8 Adagrad is {} %\".format(glob_acc5 * 100))\n",
        "print(\"The coverage for Adagrad is {} %\".format(coverage5 * 100))\n",
        "print(\"The individual accuracy for RMSprop is {} %\".format(ind_acc6 * 100))\n",
        "print(\"The sequence prediction accuracy for RMSprop is {} %\".format(glob_acc6 * 100))\n",
        "print(\"The coverage for RMSprop is {} %\".format(coverage6 * 100))\n",
        "print(\"The individual accuracy for ADAM is {} %\".format(ind_acc3 * 100))\n",
        "print(\"The sequence prediction accuracy for ADAM is {} %\".format(glob_acc3 * 100))\n",
        "print(\"The coverage for ADAM is {} %\".format(coverage3 * 100))"
      ],
      "metadata": {
        "id": "fd5ycSwkr8rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXhki-xdqFmg"
      },
      "outputs": [],
      "source": [
        "def rgb2gray(images):\n",
        "    \"\"\"Convert images from rbg to grayscale\n",
        "    \"\"\"\n",
        "    greyscale = np.dot(images, [0.2989, 0.5870, 0.1140])\n",
        "    return np.expand_dims(greyscale, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMRfQReVqFmh",
        "outputId": "9eafb633-a7ba-43fd-fa8f-90760b7583fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\abhi\\AppData\\Local\\Temp\\ipykernel_17384\\536675504.py:5: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  resize_img = np.array(image.resize((image_size[0], image_size[1]), Image.ANTIALIAS))\n"
          ]
        }
      ],
      "source": [
        "# img = input()\n",
        "img = \"image3.jpg\"\n",
        "image = Image.open(img)\n",
        "image_size = (64, 64)\n",
        "resize_img = np.array(image.resize((image_size[0], image_size[1]), Image.ANTIALIAS))\n",
        "grey_img = rgb2gray(resize_img).astype(np.float32)\n",
        "expanded_img = np.expand_dims(grey_img, axis=0)\n",
        "processed_img = prep_data_keras(expanded_img)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model7 = load_model(\"/content/drive/MyDrive/saved_model/cnn_model1\")"
      ],
      "metadata": {
        "id": "5VV8I0bern62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHWbrbtMqFmh",
        "outputId": "7aea7a0d-80e2-46bb-9eae-4cd2f5ed3282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 36ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model7.predict(processed_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQV9oAGPqFmi",
        "outputId": "d292648e-abee-486e-948d-74f87930fac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The predicted number sequence: 5 5 0\n"
          ]
        }
      ],
      "source": [
        "predicted_digits = \"\"\n",
        "for i in range(5):\n",
        "    digit = np.argmax(predictions[i][0])\n",
        "    predicted_digits += str(digit) + \" \" if digit != 10 else \"\"\n",
        "print(f'The predicted number sequence: {predicted_digits.strip(\" \")}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}